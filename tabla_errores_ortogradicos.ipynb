{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CharacterSwap:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def insert_misspelling(self, word, idx):\n",
    "        chars = [*word]\n",
    "        n_chars = len(word)\n",
    "        if idx < n_chars - 1:\n",
    "            temp_char = chars[idx]\n",
    "            chars[idx] = chars[idx + 1]\n",
    "            chars[idx + 1] = temp_char\n",
    "        else:\n",
    "            temp_char = chars[idx]\n",
    "            chars[idx] = chars[idx - 1]\n",
    "            chars[idx - 1] = temp_char\n",
    "        return \"\".join(chars)\n",
    "\n",
    "class CharacterDuplicate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def insert_misspelling(self, word, idx):\n",
    "        chars = [*word]\n",
    "        n_chars = len(word)\n",
    "        chars.insert(idx, chars[idx])\n",
    "        return \"\".join(chars)\n",
    "\n",
    "class CharacterDelete:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def insert_misspelling(self, word, idx):\n",
    "        chars = [*word]\n",
    "        n_chars = len(word)\n",
    "        del chars[idx]\n",
    "        return \"\".join(chars)\n",
    "\n",
    "\n",
    "class CharacterKeyboard:\n",
    "    def __new__(cls):\n",
    "        if not hasattr(cls, \"instance\"):\n",
    "            cls.instance = super(CharacterKeyboard, cls).__new__(cls)\n",
    "        return cls.instance\n",
    "\n",
    "    def __init__(self):\n",
    "        with open('./keybord_es.json', 'r') as f:\n",
    "            self.keyboard_misspellings = json.load(f)\n",
    "\n",
    "    def insert_misspelling(self, word, idx):\n",
    "        chars = [*word]\n",
    "        n_chars = len(word)\n",
    "        misspellings = self.keyboard_misspellings[chars[idx]]\n",
    "        chars[idx] = random.choice(misspellings)\n",
    "\n",
    "        return \"\".join(chars)\n",
    "\n",
    "class CharacterMS:\n",
    "    def __init__(self, char_max = 2, char_probs = [0.89,0.11]):\n",
    "        self.char_max = char_max,\n",
    "        self.char_probs = char_probs\n",
    "\n",
    "    def get_idxes(self, word):\n",
    "        [num_idxes]  = random.choices([1,2], weights= self.char_probs)\n",
    "        self.chars = [*word]\n",
    "        self.n_chars = len(word)\n",
    "        idxes = random.sample(list(range(self.n_chars)), k = num_idxes)\n",
    "        return idxes\n",
    "\n",
    "    def generate_misspellings(self, word):\n",
    "        idxs = self.get_idxes(word)\n",
    "        ms_types = random.choices([1,2,3,4], k= len(idxs))\n",
    "        for idx, ms_type in zip(idxs, ms_types):\n",
    "            word = self.insert_misspelling(word, idx, ms_type)\n",
    "        \n",
    "        return word\n",
    "\n",
    "    def insert_misspelling(self, word, idx, ms_type):\n",
    "        if ms_type == 1:\n",
    "            return CharacterSwap().insert_misspelling(word, idx)\n",
    "        elif ms_type == 2:\n",
    "            return CharacterDuplicate().insert_misspelling(word, idx)\n",
    "        elif ms_type == 3:\n",
    "            return CharacterDelete().insert_misspelling(word, idx)\n",
    "        elif ms_type == 4:\n",
    "            return CharacterKeyboard().insert_misspelling(word, idx)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anthony/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import random\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def find_span_token(text, tokens, tok_idx):\n",
    "    token = tokens[tok_idx]\n",
    "    \n",
    "    if tok_idx > 0:\n",
    "        start_find_span = len(\"\".join(tokens[0:tok_idx - 1]))\n",
    "    else:\n",
    "        start_find_span = 0\n",
    "    \n",
    "    match_token = re.search(token, text[start_find_span:])\n",
    "\n",
    "    if match_token:\n",
    "        span_start, span_end = match_token.span()\n",
    "        return (span_start + start_find_span, span_end + start_find_span)\n",
    "    \n",
    "    return None\n",
    "\n",
    "## tokenizar\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [str(token) for token in nlp(text)]\n",
    "    return tokens\n",
    "\n",
    "class MisspellingsAug:\n",
    "    def __init__(self, aug_pct = 0.5, tok_pct = 0.1):\n",
    "        self.aug_pct = aug_pct\n",
    "        self.tok_pct = tok_pct\n",
    "        self.load_natural_misspellings()\n",
    "\n",
    "    def load_natural_misspellings(self):\n",
    "        with open('errores_ortograficos.json', \"r\", encoding= \"utf-8\") as f:\n",
    "            errores_ortograficos = json.load(f)\n",
    "\n",
    "        self.word2missp = {}\n",
    "\n",
    "        for e in errores_ortograficos:\n",
    "            word = e['palabra']\n",
    "            misspellings = e['errores_ortograficos']\n",
    "            self.word2missp[word] = misspellings\n",
    "    \n",
    "    def get_words_with_natural_misspellings(self):\n",
    "        return [ k for k,v in self.word2missp.items() if len(v) > 0 ]\n",
    "\n",
    "    def augument_texts(self, texts , stop_words):\n",
    "        aug_size = np.ceil(self.aug_pct * len(texts))\n",
    "        #print(\"aug_size:\", aug_size)\n",
    "        texts_choices = random.sample(texts, k = int(aug_size))\n",
    "        aug_texts = []\n",
    "        ## Hacer una funcion find span token y esto usar para reemplazar o para modificar el span del otro\n",
    "        words_replace_log = []\n",
    "\n",
    "        for text in texts_choices:\n",
    "            tokens = tokenize(text)\n",
    "            idx2token  = { str(idx):token for idx, token in enumerate(tokens) if not token in stop_words}\n",
    "            num_tokens = len(idx2token)\n",
    "            num_misspellings = int(np.ceil(tok_pct * num_tokens))\n",
    "\n",
    "            words_with_natural_miss = self.get_words_with_natural_misspellings()\n",
    "\n",
    "            idxs_tok_wnm = [ int(idx) for idx, tok  in idx2token.items() if tok in words_with_natural_miss]\n",
    "            \n",
    "            idxs_tok_not_wnm = [ int(idx) for idx, tok  in idx2token.items() if not tok in words_with_natural_miss]\n",
    "\n",
    "            types_misspellings = random.choices([1,2], k = num_misspellings)\n",
    "            misspelling_words_replace = []\n",
    "\n",
    "            for type_ms in types_misspellings:\n",
    "                \n",
    "                if len(idxs_tok_wnm) == 0:\n",
    "                    type_ms = 2\n",
    "\n",
    "                if type_ms == 1:\n",
    "                    \n",
    "                    idx_elem = random.choice(list(range(len(idxs_tok_wnm ))))\n",
    "                    #print(\"idxs_tok_wnm:\", idxs_tok_wnm)\n",
    "                    #print(\"idx_elem:\", idx_elem)\n",
    "                    ## Index of token \n",
    "                    idx = idxs_tok_wnm[idx_elem]\n",
    "                    ## get misspelling word\n",
    "                    word = tokens[idx]\n",
    "                    ms_word = self.insert_natural_misspelling(word.lower())\n",
    "                    \n",
    "                    span_token = find_span_token(text, tokens, idx)\n",
    "                    misspelling_words_replace.append({\"word\": word ,\"ms_word\": ms_word, \"span_word\":  span_token})\n",
    "\n",
    "                    #tokens[idx] = self.insert_natural_misspelling(word)\n",
    "                    \n",
    "                    del idxs_tok_wnm[idx_elem]\n",
    "\n",
    "                if type_ms == 2:\n",
    "                    idx_tokens = idxs_tok_wnm + idxs_tok_not_wnm\n",
    "                    idx_elem = random.choice(list(range(len( idx_tokens ))))\n",
    "                    #print(\"idx_tok_wnm:\", idx_tok_wnm)\n",
    "                    #print(\"idx_elem:\", idx_elem)\n",
    "                    ## Index of token \n",
    "                    idx = idx_tokens[idx_elem]\n",
    "                    #tokens[idx] = self.insert_synthetic_misspelling(word)\n",
    "                    \n",
    "                    ## get misspelling word\n",
    "                    word = tokens[idx]\n",
    "                    ms_word = self.insert_synthetic_misspelling(word.lower())\n",
    "                    \n",
    "                    span_token = find_span_token(text, tokens, idx)\n",
    "\n",
    "                    misspelling_words_replace.append({\"word\": word ,\"ms_word\": ms_word, \"span_word\":  span_token})\n",
    "\n",
    "                    if len(idxs_tok_wnm) == 0 or len(idxs_tok_wnm) <= idx_elem:\n",
    "                        del idxs_tok_not_wnm[idx_elem - len(idxs_tok_wnm)]\n",
    "                    else:\n",
    "                        del idxs_tok_wnm[idx_elem]      \n",
    "\n",
    "            ## Replace misspelling in text\n",
    "            \n",
    "            if len(misspelling_words_replace) > 0:\n",
    "                misspelling_words_replace.sort(key= lambda ms: ms['span_word'][0])\n",
    "                new_text = text[ : misspelling_words_replace[0]['span_word'][0]]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            for n, misspelling in  enumerate(misspelling_words_replace):\n",
    "                ms_word = misspelling['ms_word']\n",
    "                span_start_word, span_end_word  = misspelling['span_word']\n",
    "                if n == (len(misspelling_words_replace) -1):\n",
    "                    new_text = new_text + ms_word + text[span_end_word:]\n",
    "                else:\n",
    "                    new_text = new_text + ms_word + text[span_end_word : misspelling_words_replace[n + 1]['span_word'][0]]\n",
    "                ## Update entities espan\n",
    "\n",
    "            #new_text = \" \".join(tokens)\n",
    "            words_replace_log.append({\"text\": text, \"aug_text\": new_text , \"misspelling_words_replace\": misspelling_words_replace})\n",
    "            aug_texts.append(new_text)\n",
    "        return aug_texts, words_replace_log\n",
    "\n",
    "    def insert_misspelling(self, word, ms_type):\n",
    "        idx = random.choice(list(range(len(word))))\n",
    "        \n",
    "        if ms_type == 1:\n",
    "            return CharacterSwap().insert_misspelling(word, idx)\n",
    "        elif ms_type == 2:\n",
    "            return CharacterDuplicate().insert_misspelling(word, idx)\n",
    "        elif ms_type == 3:\n",
    "            return CharacterDelete().insert_misspelling(word, idx)\n",
    "        elif ms_type == 4:\n",
    "            return CharacterKeyboard().insert_misspelling(word, idx)\n",
    "        \n",
    "    def insert_synthetic_misspelling(self, word , char_max = 2, char_probs = [0.89,0.11]):\n",
    "        [num_idxes]  = random.choices(list(range(1,char_max + 1)), weights= char_probs, k=1)\n",
    "        chars = [*word]\n",
    "        n_chars = len(word)\n",
    "        #idxes = random.sample(list(range(n_chars)), k = num_idxes)\n",
    "        ms_char_types = random.choices([1,2,3,4], k = num_idxes)\n",
    "        \n",
    "        for ms_type in ms_char_types:\n",
    "            word = self.insert_misspelling(word, ms_type)\n",
    "        return word\n",
    "\n",
    "    def insert_natural_misspelling(self, word):\n",
    "        misspellings = self.word2missp[word]\n",
    "        \n",
    "        if len(misspellings) > 0:\n",
    "            misspelling = random.choice(misspellings)\n",
    "            return misspelling\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size train data: 1272\n",
      "Size test data: 318\n"
     ]
    }
   ],
   "source": [
    "from utils import load_json\n",
    "path_train = \"./data/train_nlu.json\"\n",
    "train_data = load_json(path_train)\n",
    "\n",
    "path_test = \"./data/test_nlu.json\"\n",
    "test_data = load_json(path_test)\n",
    "\n",
    "print(\"Size train data:\", len(train_data))\n",
    "\n",
    "print(\"Size test data:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16331/1773659687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mtok_pct\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;31m# porcentaje de numero maximo de tokens reemplado en el texto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0maug_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugument_texts_intents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_pct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0maug_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugument_texts_intents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_pct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16331/1773659687.py\u001b[0m in \u001b[0;36maugument_texts_intents\u001b[0;34m(data, aug_pct, tok_pct)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mintent_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintent_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m## Aumentar con errores ortograficos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0maug_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_replace_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmisspellingsAug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugument_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mintent_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mintent_with_entities_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mintent_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intent_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mintent_with_entities_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintent_with_entities_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16331/2515940037.py\u001b[0m in \u001b[0;36maugument_texts\u001b[0;34m(self, texts, stop_words)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;31m## get misspelling word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mms_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_synthetic_misspelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mspan_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_span_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16331/2515940037.py\u001b[0m in \u001b[0;36minsert_synthetic_misspelling\u001b[0;34m(self, word, char_max, char_probs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mms_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mms_char_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_misspelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16331/2515940037.py\u001b[0m in \u001b[0;36minsert_misspelling\u001b[0;34m(self, word, ms_type)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCharacterDelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_misspelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mms_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mCharacterKeyboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_misspelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsert_synthetic_misspelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mchar_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.89\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16331/41059741.py\u001b[0m in \u001b[0;36minsert_misspelling\u001b[0;34m(self, word, idx)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mn_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mmisspellings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyboard_misspellings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisspellings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import copy\n",
    "from utils import save_json\n",
    "## fixed random \n",
    "random.seed(42)\n",
    "\n",
    "def augument_texts_intents(data, aug_pct = 0.6, tok_pct = 0.3):\n",
    "    ## Aumentar errores otograficos\n",
    "\n",
    "    ## Obtener stopwords\n",
    "    stop_words = stopwords.words('spanish')\n",
    "\n",
    "    stop_words.extend([\"?\",\".\", \",\",\"¿\"])\n",
    "\n",
    "    question_words = ['qué','que','quién','quien', 'cuál','cual', 'dónde','donde', 'cuándo','cuando', 'cómo','como', 'por qué','por que','porqué','porque', \n",
    "    'cuánto','cuanto','cuantos']\n",
    "\n",
    "    ## remover palabras de preguntas de stopwords\n",
    "    stop_words = [ sw for sw in stop_words if not sw in question_words ]\n",
    "\n",
    "    ## Generador de errores ortográficos\n",
    "\n",
    "    misspellingsAug = MisspellingsAug(aug_pct, tok_pct)\n",
    "\n",
    "    ## Train data augumenter misspelling\n",
    "\n",
    "    intents = list(set(([sample['intent_name'] for sample in data])))\n",
    "\n",
    "    aug_data = []\n",
    "\n",
    "    for intent_name in intents:\n",
    "        intent_examples = [ sample for sample in data if intent_name == sample['intent_name']]\n",
    "        intent_texts = [ sample['text'] for sample in intent_examples]\n",
    "        ## Aumentar con errores ortograficos\n",
    "        aug_texts, words_replace_log = misspellingsAug.augument_texts(texts= intent_texts, stop_words = stop_words)\n",
    "        intent_with_entities_examples = [ sample for sample in data if intent_name == sample['intent_name'] and len(sample['entities']) > 0]\n",
    "        intent_with_entities_texts = [ sample['text'] for sample in intent_with_entities_examples]\n",
    "\n",
    "        for aug_idx , aug_words_log in enumerate(words_replace_log):\n",
    "            if len(aug_words_log) == 0:\n",
    "                continue\n",
    "            \n",
    "            if aug_words_log['text'] in intent_with_entities_texts:\n",
    "                idx_sample = intent_with_entities_texts.index(aug_words_log['text'])\n",
    "                intent_sample  = intent_with_entities_examples[idx_sample]\n",
    "                new_sample = copy.deepcopy(intent_sample)\n",
    "                new_sample['text'] = aug_texts[aug_idx]\n",
    "                \n",
    "                entities = intent_sample['entities']\n",
    "                new_entities = new_sample['entities']\n",
    "                \n",
    "                if len(entities) > 0:\n",
    "                \n",
    "                    for aug_log_word in aug_words_log['misspelling_words_replace']:\n",
    "                        displace = 0\n",
    "                        word = aug_log_word['word']\n",
    "                        ms_word = aug_log_word['ms_word']\n",
    "                        displace = displace + len(ms_word) - len(word)\n",
    "                        for ent, new_ent in zip(entities, new_entities):\n",
    "                            if ent['span_start'] > aug_log_word['span_word'][0]:\n",
    "                                new_ent['span_start']  = displace + new_ent['span_start']\n",
    "                                new_ent['span_end']  = displace + new_ent['span_end']\n",
    "                            else:\n",
    "                                if aug_log_word['span_word'][1] <= ent['span_end']:\n",
    "                                    new_ent['span_end'] = displace + new_ent['span_end']\n",
    "                    \n",
    "                    for ent in new_entities:\n",
    "                        ent['value'] = aug_texts[aug_idx][ent['span_start']:ent['span_end']]\n",
    "                    \n",
    "                    new_sample['entities'] = new_entities\n",
    "                \n",
    "                aug_data.append(new_sample)\n",
    "                \n",
    "            else: \n",
    "                new_sample =  {\n",
    "                \"text\": aug_texts[aug_idx] ,\n",
    "                \"intent_name\": intent_name,\n",
    "                \"entities\": [],\n",
    "                \"text_label_entities\": \"\"\n",
    "                }\n",
    "                aug_data.append(new_sample)\n",
    "\n",
    "    return aug_data \n",
    "\n",
    "aug_pct = 0.6 # porcentaje de numero maximo en textos escogidos\n",
    "tok_pct  = 0.3 # porcentaje de numero maximo de tokens reemplado en el texto\n",
    "\n",
    "aug_train_data = augument_texts_intents(train_data, aug_pct, tok_pct)\n",
    "\n",
    "aug_test_data = augument_texts_intents(test_data, aug_pct, tok_pct)\n",
    "\n",
    "all_train_data = train_data + aug_train_data \n",
    "save_json(all_train_data, \"data/aug_train_nlu.json\")\n",
    "\n",
    "all_test_data = test_data + aug_test_data\n",
    "save_json(all_test_data, \"data/aug_test_nlu.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displace: 0\n",
      "aug_log_word {'word': 'especial', 'span_word': (18, 26), 'ms_word': 'dspecail'}\n",
      "new_entities: [{'value': 'retiro total', 'entity': 'procedimiento', 'span_start': 35, 'span_end': 47}]\n",
      "ent: {'value': 'retiro otal', 'entity': 'procedimiento', 'span_start': 35, 'span_end': 46}\n",
      "displace: -1\n",
      "aug_log_word {'word': 'total', 'span_word': (42, 47), 'ms_word': 'otal'}\n",
      "new_entities: [{'value': 'retiro total', 'entity': 'procedimiento', 'span_start': 35, 'span_end': 47}]\n",
      "ent: {'value': 'retiro total', 'entity': 'procedimiento', 'span_start': 35, 'span_end': 47}\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "intent_sample = {\"text\": \"hay algun formato especial para el retiro total?\",\n",
    "            \"entities\" : [{'value': 'retiro total',\n",
    "            'entity': 'procedimiento',\n",
    "            'span_start': 35,\n",
    "            'span_end': 47}]}\n",
    "\n",
    "text = intent_sample[\"text\"]\n",
    "aug_text =\"hay algun formato dspecail para el retiro otal?\"\n",
    "tokens =tokenize(text)\n",
    "find_span_token(text, tokens, 3)\n",
    "\n",
    "word_logs = [{\n",
    "    \"word\": tokens[3],\n",
    "    \"span_word\": find_span_token(text, tokens, 3),\n",
    "    \"ms_word\": \"dspecail\"\n",
    "},{\n",
    "    \"word\": tokens[7],\n",
    "    \"span_word\": find_span_token(text, tokens, 7),\n",
    "    \"ms_word\": \"otal\"  \n",
    "}]\n",
    "\n",
    "new_sample = copy.deepcopy(intent_sample)\n",
    "\n",
    "entities = intent_sample['entities']\n",
    "new_entities = new_sample['entities']\n",
    "\n",
    "for aug_log_word in word_logs:\n",
    "    displace = 0\n",
    "    word = aug_log_word['word']\n",
    "    ms_word = aug_log_word['ms_word']\n",
    "    displace = displace + len(ms_word) - len(word)\n",
    "    print(\"displace:\", displace)\n",
    "    print(\"aug_log_word\", aug_log_word)\n",
    "    print(\"new_entities:\", new_entities)\n",
    "    print(\"ent:\", ent)\n",
    "    for ent, new_ent in zip(entities, new_entities):\n",
    "        if ent['span_start'] > aug_log_word['span_word'][0]:\n",
    "            new_ent['span_start']  = displace + new_ent['span_start']\n",
    "            new_ent['span_end']  = displace + new_ent['span_end']\n",
    "        else:\n",
    "            if aug_log_word['span_word'][1] <= ent['span_end']:\n",
    "                new_ent['span_end'] = displace + new_ent['span_end']\n",
    "\n",
    "for ent in new_entities:\n",
    "    ent['value'] = aug_text[ent['span_start']:ent['span_end']]\n",
    "\n",
    "new_sample['entities'] = new_entities\n",
    "new_sample['text'] = aug_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85f4d604cc416e640273d1c6062925d4c51dc9b6da6b9ead7493253ae9a81b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
