{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "ENTITY_REGEX = re.compile(\n",
    "    r\"\\[(?P<entity_text>[^\\]]+?)\\](\\((?P<entity>[^:)]+?)(?:\\:(?P<value>[^)]+))?\\)|\\{(?P<entity_dict>[^}]+?)\\}|\\[(?P<list_entity_dicts>.*?)\\])\"  # noqa: E501, W505\n",
    ")\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    offset = 0 \n",
    "    \n",
    "    for match in re.finditer(ENTITY_REGEX, text):\n",
    "        entity_text = match.groupdict()[\"entity_text\"]\n",
    "        entity_dict_str = match.groupdict()[\"entity_dict\"]\n",
    "        entity_dict = json.loads(f\"{{{entity_dict_str}}}\")\n",
    "        entity_type = entity_dict[\"entity\"]\n",
    "        start_index = match.start() - offset\n",
    "        end_index = start_index + len(entity_text)\n",
    "        offset += len(match.group(0)) - len(entity_text)\n",
    "\n",
    "        entities.append({\n",
    "            \"start_span\": start_index,\n",
    "            \"end_span\": end_index,\n",
    "            \"value\": entity_text,\n",
    "            \"entity\": entity_type\n",
    "        })\n",
    "    return entities\n",
    "\n",
    "def convert_plain_text(text):\n",
    "    plain_text = re.sub(ENTITY_REGEX, lambda m: m.groupdict()[\"entity_text\"], text)\n",
    "\n",
    "    return plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_json(data_dict, name = \"data_nlu\"):\n",
    "    with open(f\"{name}.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(data_dict, f, indent= 4)\n",
    "\n",
    "def load_json(path = \"./data_nlu.json\"):\n",
    "    with open(f\"{path}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel import yaml as yaml\n",
    "filename = './data/nlu_cursos_version.yml'\n",
    "\n",
    "def read_yaml_file(filename):\n",
    "    with open(filename, encoding= \"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    yaml_parser = yaml.YAML(typ = 'safe')\n",
    "    yaml_parser.version = (1, 2)\n",
    "    yaml_parser.preserve_quotes = True\n",
    "    data_nlu = yaml_parser.load(content)\n",
    "\n",
    "    data_intents = []\n",
    "    for examples_nlu in  data_nlu['nlu']:\n",
    "        intent = examples_nlu['intent']  \n",
    "        examples = examples_nlu['examples']\n",
    "        examples_intent = []\n",
    "        for example in examples.splitlines():\n",
    "            examples_intent.append(example[1:].strip( \"\\n\\r \"))\n",
    "        \n",
    "        data_intents.append({'intent': intent, 'examples': examples_intent})\n",
    "    return data_intents\n",
    "\n",
    "intent_examples = read_yaml_file(filename)\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for intent in intent_examples:\n",
    "    intent_name = intent['intent']\n",
    "    examples = intent['examples']\n",
    "    if len(examples) < 10:\n",
    "        continue\n",
    "    for example in examples:\n",
    "        entities = extract_entities(example)\n",
    "        text = convert_plain_text(example)\n",
    "        data.append({\n",
    "            \"text\": text,\n",
    "            \"intent_name\": intent_name,\n",
    "            \"entities\": entities\n",
    "\n",
    "        })\n",
    "\n",
    "save_json(data, name = \"data/data_nlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 1378\n",
      "test size: 345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "intents_labels = [sample['intent_name'] for sample in data]\n",
    "\n",
    "train_data , test_data = train_test_split(data, random_state=42, test_size= 0.2, shuffle= True, stratify= intents_labels)\n",
    "print(\"train size:\", len(train_data))\n",
    "\n",
    "print(\"test size:\", len(test_data))\n",
    "\n",
    "save_json(train_data, name = \"data/train_nlu\")\n",
    "\n",
    "save_json(test_data, name = \"data/test_nlu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85f4d604cc416e640273d1c6062925d4c51dc9b6da6b9ead7493253ae9a81b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
